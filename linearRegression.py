import scipy.ioimport numpy as npfrom sklearn.linear_model import LinearRegressionimport matplotlib.pyplot as pltimport mathfrom sklearn.model_selection import KFold, cross_val_score'''This code uploads a matrix with .mat file format and does multiple quadratic regression. The data in the input matrix should be as follows:        y1 x11 x12 x13 ... x1n    y2 x21 x22 x33 ... x2n    y3 ...    .    .    .    yn ...    Milad Taghizadeh 08/20/2024'''# Load the .mat filemat = scipy.io.loadmat('highest_entropy_5000_samples.mat')# Extract the matrixdata_original = mat['data']data_original = np.delete(data_original, 1, axis=1) # delete the v* data## add a row of zeros# Row of zeroszero_row = np.zeros(data_original.shape[1])# Adding the row of zerosdata = np.vstack([zero_row, data_original])# a function to split data and choose samples randomly for test and traindef split_matrix(matrix, test_fraction=0.2):    # Get the number of rows in the matrix    num_rows = matrix.shape[0]        # Calculate the number of test rows    num_test_rows = int(num_rows * test_fraction)        # Randomly select indices for the test set    test_indices = np.random.choice(num_rows, num_test_rows, replace=False)        # Create test and training sets based on the indices    test_matrix = matrix[test_indices, :]    train_indices = np.setdiff1d(np.arange(num_rows), test_indices)    train_matrix = matrix[train_indices, :]        return train_matrix, test_matrix# splitting the datatrain_matrix, test_matrix = split_matrix(data)# Split the training and testing to separate independent variabletraining_P_star = train_matrix[:, 0]        training_inputs = train_matrix[:, 1:]testing_P_star = test_matrix[:, 0]testing_inputs  = test_matrix[:, 1:]## using linear regression to get coefficientsregressor = LinearRegression()# # Define the number of folds for k-fold cross-validation# k = 8# # Set up the KFold object# kf = KFold(n_splits=k, shuffle=True, random_state=1)# # Perform k-fold cross-validation and compute the scores# scores = cross_val_score(regressor, training_inputs , training_P_star , cv=kf, scoring='r2')regressor.fit(training_inputs, training_P_star)## display coefficientsc = regressor.coef_c0 = regressor.intercept_## predictionsP_star_pred = regressor.predict(testing_inputs)## RMSE calculatormean_square_error = np.square(np.subtract(testing_P_star,P_star_pred)).mean()  root_mean_square_error = math.sqrt(mean_square_error)## Compute RRMSErrmse = root_mean_square_error / np.mean(testing_P_star)##################################################################3# Define figure sizefig = plt.figure(figsize=(5, 5))  # Adjust width and height as needed# Scatter plotplt.scatter(testing_P_star, P_star_pred, color='crimson')# Set axis limits to start from 0.00plt.xlim(0.00, 0.3)plt.ylim(0.00, 0.3)# Set exactly 4 equispaced ticks on both axesplt.xticks(np.round(np.linspace(0.00, 0.3, 4), 2))plt.yticks(np.round(np.linspace(0.00, 0.3, 4), 2))# Labels and titleplt.title(f'RRMSE = {rrmse:.2g}', fontsize=18, fontname='Arial')plt.xlabel(r'Ground-truth $\bar{P}^*$', fontsize=18, fontname='Arial')plt.ylabel('Predictions', fontsize=18, fontname='Arial')# Aspect ratioplt.gca().set_aspect('equal', adjustable='box')# Unity lineplt.plot(np.linspace(0, 0.3, 100), np.linspace(0, 0.3, 100),          label='Unity Line (y = x)', color='black', linestyle='--')# Tick sizeplt.tick_params(axis='both', labelsize=18)plt.tight_layout()  # Automatically adjust padding# Show plotplt.show()## vectorize the figure#plt.savefig("Regression_linear.svg",dpi=300)